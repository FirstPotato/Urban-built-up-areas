{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data introduction and import necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pixel Resolution:** 30m * 30m<br>\n",
    "**Target:** Target data set<br>\n",
    "**Point of Interest (POI):** POI Kernel Density, \n",
    "- POI_All, BandWidth range: 250 m - 2500 m, interval = 250 m<br>\n",
    "- POI_Sel, BandWidth range: 250 m - 2500 m, interval = 250 m<br>\n",
    "- POI_All: 1000m; POI_Sel: 500m<br>\n",
    "\n",
    "**Road Network (RN):** Road Kernel Density\n",
    "- BandWidth range: 250 m - 2500 m, interval = 250 m<br>\n",
    "- BandWidth: 1250m<br>\n",
    "\n",
    "**NTL:** Time,2019/03; NPP-VIIRS, DNB<br>\n",
    "**XM_Boundary:** mask of all the layers<br>\n",
    "**Train_Test:** \n",
    "- Train, data located in Off-island area of Xiamen, label = 0\n",
    "- Test,  data located in Island area of Xiamen, label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sl_1 import * # my custom module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate header named begin.txt from layer Target\n"
     ]
    }
   ],
   "source": [
    "# Extract and generate headers\n",
    "extrat_begin('Target', 'begin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ASCII to array and generate header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(name_pref):\n",
    "    '''\n",
    "    input: name_pref, name prefixs of lyer name list\n",
    "    output: a lyer name list\n",
    "    '''\n",
    "    ly_name = [name_pref + str(i) for i in range(250,2750,250)]\n",
    "    return ly_name\n",
    "# get_name('poi_all_')  # test example 1\n",
    "# get_name('RN_') # test example 2\n",
    "# get_name('poi_all_') + get_name('RN_') + ['Target','Train_Test','XM_Boundary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def read_ASC_Data(file_name, ly_names):\n",
    "    '''input: file_name, a file name, parent file name of ly_names\n",
    "       input: ly_names, a list, list of layer names in an ASCII data format\n",
    "       output: ly_dict: a dictionary whose key is the layer name and the value is stored in an array format\n",
    "    '''     \n",
    "    ly_dict = {}\n",
    "    for name in ly_names:\n",
    "        ly_dict[name] = np.loadtxt('%s/\\%s.txt' % (file_name, name), skiprows = 6)\n",
    "    print('All ASCII data has been read')\n",
    "    return ly_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data consistency test\n",
    "Check whether there are missing values in different positions of the layer,<br>\n",
    "and auto fill it if missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Na_Test(layer_names,stan_layer,ly_dict):\n",
    "    '''input: layer_names, a list of layer names to be checked for missing values\n",
    "       input: stan_layer, the normalized layer name used to verify that other layers have missing values\n",
    "       ly_dict: the dict data read by read_ASC_Data function.\n",
    "       output: For each layer in the name list, the following judgment is made. \n",
    "               If all the row raster cells of a layer are missing from the standard layer, \n",
    "               the layer is added to the result return list, \n",
    "               otherwise, the next layer is performed. Judgment.\n",
    "    '''\n",
    "    list1 = []\n",
    "    for name in layer_names:\n",
    "        # Fill in missing values start\n",
    "        ly_dict[name][np.where(ly_dict[name] == -9999)] = 0 # Fill all missing values (- 9999) with 0\n",
    "        ly_dict[name][np.where(ly_dict[stan_layer] == -9999)] = -9999 # Make - 9999 consistent with the standardized layer\n",
    "        # Fill in missing values end\n",
    "        a = max(ly_dict[name][np.where(ly_dict[stan_layer] == -9999)])\n",
    "        b = min(ly_dict[name][np.where(ly_dict[stan_layer] != -9999)])\n",
    "        if a == -9999 and b != -9999:\n",
    "            continue\n",
    "        else:\n",
    "            list1.append(name)\n",
    "    if len(list1) == 0:\n",
    "        print('All layers pass the inspection, no missing values exist, and are consistent with the standardized layer')\n",
    "    else:\n",
    "        print('The following layers have missing values')\n",
    "        print(list1)\n",
    "    return\n",
    "### test\n",
    "# test_name = get_name('poi_all_')\n",
    "# ly_dict = copy.deepcopy(read_ASC_Data('ASCII', test_name))\n",
    "# Na_Test(test_name[0],'XM_Boundary',ly_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revel_array(dict_one):\n",
    "    '''input: dict_one, a dictionary whose key is the layer name and the value is stored in an array format\n",
    "       output: A dict whose value is all converted to one-dimensional array\n",
    "    '''\n",
    "    new_dict = copy.deepcopy(dict_one)\n",
    "    for key,value in new_dict.items():\n",
    "        new_dict[key] = np.ravel(value, order='C') # Expand to one dimension by row\n",
    "    num_rows = len(list(new_dict.values())[0])\n",
    "    new_dict['ID'] = np.array([i for i in range(num_rows)]) # Build index with name ID\n",
    "    return new_dict\n",
    "\n",
    "# ly_dict_reval = revel_array(ly_dict)\n",
    "# ly_dict_reval\n",
    "# len(ly_dict_reval['Target']) # show total number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conversion to dataframe and to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ly_df_csv(input_file_loc,ly_names,write_name):\n",
    "    '''\n",
    "    input: input_file_loc, file loction name of input data\n",
    "    input: ly_names, , a list of layer names to be combined\n",
    "    output: a csv file named write_name\n",
    "    '''\n",
    "    ly_dict0 = read_ASC_Data(input_file_loc, ly_names) # read ASCII data\n",
    "    ly_dict = copy.deepcopy(ly_dict0)\n",
    "#     print(ly_dict['Target'])\n",
    "#     print(np.shape(ly_dict['Target']))\n",
    "    test_name = ly_names\n",
    "    Na_Test(test_name[:-1],test_name[-1], ly_dict) # test_name[-1], stan_layer, in this case is 'XM_Boundary'\n",
    "    ly_dict_reval = revel_array(ly_dict) # Dimension reduction\n",
    "#     print(len(ly_dict_reval['Target'])) # show total number of records\n",
    "    ly_df = pd.DataFrame(ly_dict_reval,columns=ly_dict_reval.keys())\n",
    "    ly_df.to_csv(r'data\\%s.csv' % (write_name),index = False)\n",
    "    print('Data writing is over')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write three csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ASCII data has been read\n",
      "All layers pass the inspection, no missing values exist, and are consistent with the standardized layer\n",
      "Data writing is over\n"
     ]
    }
   ],
   "source": [
    "input_file_loc = 'ASCII'\n",
    "ly_names = get_name('poi_all_') + get_name('poi_sel_') + get_name('RN_') + ['NTL','Target','Train_Test','XM_Boundary']\n",
    "write_name = 'ly_df'\n",
    "ly_df_csv(input_file_loc,ly_names,write_name) # write the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/ly_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3450552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['poi_all_250', 'poi_all_500', 'poi_all_750', 'poi_all_1000',\n",
       "       'poi_all_1250', 'poi_all_1500', 'poi_all_1750', 'poi_all_2000',\n",
       "       'poi_all_2250', 'poi_all_2500', 'poi_sel_250', 'poi_sel_500',\n",
       "       'poi_sel_750', 'poi_sel_1000', 'poi_sel_1250', 'poi_sel_1500',\n",
       "       'poi_sel_1750', 'poi_sel_2000', 'poi_sel_2250', 'poi_sel_2500',\n",
       "       'RN_250', 'RN_500', 'RN_750', 'RN_1000', 'RN_1250', 'RN_1500',\n",
       "       'RN_1750', 'RN_2000', 'RN_2250', 'RN_2500', 'Target', 'Train_Test',\n",
       "       'XM_Boundary', 'ID', 'NTL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = ly_df.copy()\n",
    "print(len(data))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi_all_250</th>\n",
       "      <th>poi_all_500</th>\n",
       "      <th>poi_all_750</th>\n",
       "      <th>poi_all_1000</th>\n",
       "      <th>poi_all_1250</th>\n",
       "      <th>poi_all_1500</th>\n",
       "      <th>poi_all_1750</th>\n",
       "      <th>poi_all_2000</th>\n",
       "      <th>poi_all_2250</th>\n",
       "      <th>poi_all_2500</th>\n",
       "      <th>...</th>\n",
       "      <th>RN_1500</th>\n",
       "      <th>RN_1750</th>\n",
       "      <th>RN_2000</th>\n",
       "      <th>RN_2250</th>\n",
       "      <th>RN_2500</th>\n",
       "      <th>Target</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "      <th>NTL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.025637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2361</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.022329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2362</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.019295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2363</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      poi_all_250  poi_all_500  poi_all_750  poi_all_1000  poi_all_1250  \\\n",
       "430           0.0          0.0          0.0           0.0           0.0   \n",
       "431           0.0          0.0          0.0           0.0           0.0   \n",
       "2361          0.0          0.0          0.0           0.0           0.0   \n",
       "2362          0.0          0.0          0.0           0.0           0.0   \n",
       "2363          0.0          0.0          0.0           0.0           0.0   \n",
       "\n",
       "      poi_all_1500  poi_all_1750  poi_all_2000  poi_all_2250  poi_all_2500  \\\n",
       "430            0.0           0.0           0.0      0.002976      0.020894   \n",
       "431            0.0           0.0           0.0      0.001634      0.018002   \n",
       "2361           0.0           0.0           0.0      0.005609      0.025637   \n",
       "2362           0.0           0.0           0.0      0.003711      0.022329   \n",
       "2363           0.0           0.0           0.0      0.002184      0.019295   \n",
       "\n",
       "      ...  RN_1500  RN_1750  RN_2000  RN_2250  RN_2500  Target  Train_Test  \\\n",
       "430   ...      0.0      0.0      0.0      0.0      0.0     0.0         0.0   \n",
       "431   ...      0.0      0.0      0.0      0.0      0.0     0.0         0.0   \n",
       "2361  ...      0.0      0.0      0.0      0.0      0.0     0.0         0.0   \n",
       "2362  ...      0.0      0.0      0.0      0.0      0.0     0.0         0.0   \n",
       "2363  ...      0.0      0.0      0.0      0.0      0.0     0.0         0.0   \n",
       "\n",
       "      XM_Boundary    ID     NTL  \n",
       "430           1.0   430 -9999.0  \n",
       "431           1.0   431 -9999.0  \n",
       "2361          1.0  2361 -9999.0  \n",
       "2362          1.0  2362 -9999.0  \n",
       "2363          1.0  2363 -9999.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = data.copy()\n",
    "clean_data = clean_data[clean_data.loc[:,'Target'] != -9999]\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1890571"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add lng and lat of each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118.011595,  24.907131],\n",
       "       [118.011892,  24.907129],\n",
       "       [118.011296,  24.906862],\n",
       "       [118.011593,  24.90686 ],\n",
       "       [118.01189 ,  24.906858],\n",
       "       [118.012187,  24.906856]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coor = np.loadtxt('data/coor.txt') # load coordinate data\n",
    "coor[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1890571"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['lng'] = coor[:,0]\n",
    "clean_data['lat'] = coor[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['poi_all_250', 'poi_all_500', 'poi_all_750', 'poi_all_1000',\n",
       "       'poi_all_1250', 'poi_all_1500', 'poi_all_1750', 'poi_all_2000',\n",
       "       'poi_all_2250', 'poi_all_2500', 'poi_sel_250', 'poi_sel_500',\n",
       "       'poi_sel_750', 'poi_sel_1000', 'poi_sel_1250', 'poi_sel_1500',\n",
       "       'poi_sel_1750', 'poi_sel_2000', 'poi_sel_2250', 'poi_sel_2500',\n",
       "       'RN_250', 'RN_500', 'RN_750', 'RN_1000', 'RN_1250', 'RN_1500',\n",
       "       'RN_1750', 'RN_2000', 'RN_2250', 'RN_2500', 'Target', 'Train_Test',\n",
       "       'XM_Boundary', 'ID', 'NTL', 'lng', 'lat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder and save clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "order = ['poi_all_250', 'poi_all_500', 'poi_all_750', 'poi_all_1000',\n",
    "       'poi_all_1250', 'poi_all_1500', 'poi_all_1750', 'poi_all_2000',\n",
    "       'poi_all_2250', 'poi_all_2500', 'poi_sel_250', 'poi_sel_500',\n",
    "       'poi_sel_750', 'poi_sel_1000', 'poi_sel_1250', 'poi_sel_1500',\n",
    "       'poi_sel_1750', 'poi_sel_2000', 'poi_sel_2250', 'poi_sel_2500',\n",
    "       'RN_250', 'RN_500', 'RN_750', 'RN_1000', 'RN_1250', 'RN_1500',\n",
    "       'RN_1750', 'RN_2000', 'RN_2250', 'RN_2500', 'NTL',\n",
    "       'lng', 'lat', 'Target', 'Train_Test', 'XM_Boundary', 'ID']\n",
    "clean_data = clean_data[order]\n",
    "# clean_data.rename(columns={'POI_Sel':'POI'}, inplace = True)\n",
    "clean_data.to_csv(r'data\\ly_df_clean.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read clean data and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi_all_250</th>\n",
       "      <th>poi_all_500</th>\n",
       "      <th>poi_all_750</th>\n",
       "      <th>poi_all_1000</th>\n",
       "      <th>poi_all_1250</th>\n",
       "      <th>poi_all_1500</th>\n",
       "      <th>poi_all_1750</th>\n",
       "      <th>poi_all_2000</th>\n",
       "      <th>poi_all_2250</th>\n",
       "      <th>poi_all_2500</th>\n",
       "      <th>...</th>\n",
       "      <th>RN_2000</th>\n",
       "      <th>RN_2250</th>\n",
       "      <th>RN_2500</th>\n",
       "      <th>NTL</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>Target</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>118.011595</td>\n",
       "      <td>24.907131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>118.011892</td>\n",
       "      <td>24.907129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.025637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>118.011296</td>\n",
       "      <td>24.906862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.022329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>118.011593</td>\n",
       "      <td>24.906860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.019295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>118.011890</td>\n",
       "      <td>24.906858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   poi_all_250  poi_all_500  poi_all_750  poi_all_1000  poi_all_1250  \\\n",
       "0          0.0          0.0          0.0           0.0           0.0   \n",
       "1          0.0          0.0          0.0           0.0           0.0   \n",
       "2          0.0          0.0          0.0           0.0           0.0   \n",
       "3          0.0          0.0          0.0           0.0           0.0   \n",
       "4          0.0          0.0          0.0           0.0           0.0   \n",
       "\n",
       "   poi_all_1500  poi_all_1750  poi_all_2000  poi_all_2250  poi_all_2500  ...  \\\n",
       "0           0.0           0.0           0.0      0.002976      0.020894  ...   \n",
       "1           0.0           0.0           0.0      0.001634      0.018002  ...   \n",
       "2           0.0           0.0           0.0      0.005609      0.025637  ...   \n",
       "3           0.0           0.0           0.0      0.003711      0.022329  ...   \n",
       "4           0.0           0.0           0.0      0.002184      0.019295  ...   \n",
       "\n",
       "   RN_2000  RN_2250  RN_2500     NTL         lng        lat  Target  \\\n",
       "0      0.0      0.0      0.0 -9999.0  118.011595  24.907131     0.0   \n",
       "1      0.0      0.0      0.0 -9999.0  118.011892  24.907129     0.0   \n",
       "2      0.0      0.0      0.0 -9999.0  118.011296  24.906862     0.0   \n",
       "3      0.0      0.0      0.0 -9999.0  118.011593  24.906860     0.0   \n",
       "4      0.0      0.0      0.0 -9999.0  118.011890  24.906858     0.0   \n",
       "\n",
       "   Train_Test  XM_Boundary    ID  \n",
       "0         0.0          1.0   430  \n",
       "1         0.0          1.0   431  \n",
       "2         0.0          1.0  2361  \n",
       "3         0.0          1.0  2362  \n",
       "4         0.0          1.0  2363  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = pd.read_csv(r'data\\ly_df_clean.csv')\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_data) # Sample size: 1890571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spli train and test\n",
    "- Train, data located in Off-island area of Xiamen, label = 0\n",
    "- Test,  data located in Island area of Xiamen, label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test = train_test_split(clean_data, test_size=0.33, random_state=160) \n",
    "# orignal split but don't have spatial independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train = clean_data[clean_data['Train_Test'] == 0]\n",
    "Test = clean_data[clean_data['Train_Test'] == 1]\n",
    "# Train.to_csv(r'data\\Train.csv')\n",
    "# Test.to_csv(r'data\\Test.csv')\n",
    "# Train.head()\n",
    "# Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"data/POI_Sel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = data1['adname'].isin(['集美区','同安区','翔安区']) # Off-island\n",
    "a2 = data1['adname'].isin(['思明区','湖里区']) # in island\n",
    "Train_POI = data1.loc[a1]\n",
    "Test_POI = data1.loc[a2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import svm\n",
    "X = np.array(Train['poi_all_250']).reshape(-1, 1)\n",
    "y = Train['Target']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "scores = cross_validate(clf, X, y,\n",
    "                        scoring='precision_macro', cv=3,\n",
    "                        return_estimator=True)\n",
    "sorted(scores.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "X = np.array(Train['poi_all_250']).reshape(-1, 1)\n",
    "y = Train['Target']\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "cv_results = cross_validate(lasso, X, y, cv=3, return_train_score=True)\n",
    "# sorted(cv_results.keys())\n",
    "# ['fit_time', 'score_time', 'test_score', 'train_score']\n",
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def get_name(name_pref):\n",
    "    '''\n",
    "    input: name_pref, name prefixs of lyer name list\n",
    "    output: a lyer name list\n",
    "    '''\n",
    "    ly_name = [name_pref + str(i) for i in range(250,2750,250)]\n",
    "    return ly_name\n",
    "# get_name('poi_all_')  # test example 1\n",
    "# get_name('RN_') # test example 2\n",
    "# get_name('poi_all_') + get_name('RN_') + ['Target','Train_Test','XM_Boundary']\n",
    "\n",
    "def opti_band(names):\n",
    "    '''input: names, a list made up of different band width name\n",
    "       output: a string, a band width name with max score\n",
    "    '''\n",
    "    score_list = []\n",
    "    for name in names:\n",
    "        X = np.array(Train[name]).reshape(-1, 1)\n",
    "        y = Train['Target']\n",
    "        clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "        scores = cross_val_score(clf, X, y, cv=5)\n",
    "        score_list.append(scores.mean())\n",
    "    return score_list\n",
    "\n",
    "def select_name(names):\n",
    "    A = {}\n",
    "    scores = opti_band(names)\n",
    "    ind = scores.index(max(scores))\n",
    "    name = names[ind]\n",
    "    A[\"name\"] = name\n",
    "    A[\"scores\"] = scores\n",
    "    return A\n",
    "# input_name = 'poi_all_' # \n",
    "# names = get_name(input_name)[0:2]\n",
    "# name = select_name(names)\n",
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'poi_all_250', 'scores': [0.837207688762503, 0.8355518447889045]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for input_name in ['poi_sel_250', 'RN_500']:\n",
    "    names = get_name(input_name)[0:2]\n",
    "    name = select_name(names)\n",
    "    name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8591486166330017\n",
      "0.8435773701765518\n",
      "0.8834397611050637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "for name in ['poi_sel_250', 'RN_500','NTL']:\n",
    "    X_Train =  np.array(Train[name]).reshape(-1, 1)\n",
    "    Y_Train = Train['Target'].copy()\n",
    "    X_Test = np.array(Test[name]).reshape(-1, 1)\n",
    "    Y_Test = Test['Target'].copy() # get sub dataframe\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=2, random_state=1)\n",
    "    clf.fit(X_Train, Y_Train)\n",
    "    predictions = clf.predict(X_Test)\n",
    "    acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf_side: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "X_Train =  np.array(Train[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Train = Train['Target'].copy()\n",
    "params = {'max_leaf_nodes': list(range(2, 10))}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=5, iid=False)\n",
    "grid.fit(X_Train, Y_Train)\n",
    "print(\"leaf_side: {0}\".format(grid.best_estimator_.max_leaf_nodes))\n",
    "# use the best estimator to compute the kernel density estimate\n",
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8479023930240815\n"
     ]
    }
   ],
   "source": [
    "# X_Train =  np.array(Train[['poi_sel_250', 'RN_500','NTL']])\n",
    "# Y_Train = Train['Target'].copy()\n",
    "# X_Test = np.array(Test[['poi_sel_250', 'RN_500','NTL']])\n",
    "# Y_Test = Test['Target'].copy() # get sub dataframe\n",
    "# clf = DecisionTreeClassifier(max_leaf_nodes=4, random_state=1)\n",
    "# clf.fit(X_Train, Y_Train)\n",
    "predictions = clf.predict(X_Test)\n",
    "acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "- RandomForest\n",
    "- AdaBoost\n",
    "- Gradient Tree Boosting\n",
    "- Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "names = [\"LogisticRegression\",\n",
    "         \"Decision Tree\", \n",
    "         \"Random Forest\", \n",
    "         \"Neural Net\", \n",
    "         \"AdaBoost\",\n",
    "         \"Naive Bayes\", \n",
    "         \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(penalty='l2', solver='sag',max_iter =150),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=50),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier LogisticRegression overall_accuracy: 0.8391766739817569 f1_score 0.8082362025798979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_Train =  np.array(Train[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Train = Train['Target'].copy()\n",
    "X_Test = np.array(Test[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Test = Test['Target'].copy() # get sub dataframe\n",
    "\n",
    "\n",
    "for name, clf in list(zip(names, classifiers))[1:2]:\n",
    "    clf.fit(X_Train, Y_Train)\n",
    "    score = clf.score(X_Test, Y_Test) # overall_accuracy\n",
    "    predictions = clf.predict(X_Test)\n",
    "    f1 = f1_score(Y_Test, predictions, average='macro') # f1_score\n",
    "    print('classifier', name, \"overall_accuracy:\", score,'f1_score',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709885849666165"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1 0.01 0.001 1e-4 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"RandomForest\", RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)),\n",
    "              (\"AdaBoost\", AdaBoostClassifier(n_estimators=100)),\n",
    "              (\"Gradient Tree Boosting\", GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0))\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_name(name_pref):\n",
    "    '''\n",
    "    input: name_pref, name prefixs of lyer name list\n",
    "    output: a lyer name list\n",
    "    '''\n",
    "    ly_name = [name_pref + str(i) for i in range(250,2750,250)]\n",
    "    return ly_name\n",
    "# get_name('poi_all_')  # test example 1\n",
    "# get_name('RN_') # test example 2\n",
    "# get_name('poi_all_') + get_name('RN_') + ['Target','Train_Test','XM_Boundary']\n",
    "\n",
    "def opti_band(names):\n",
    "    '''input: names, a list made up of different band width name\n",
    "       output: a string, a band width name with max score\n",
    "    '''\n",
    "    score_list = []\n",
    "    for name in names:\n",
    "        X = np.array(Train[name]).reshape(-1, 1)\n",
    "        y = Train['Target']\n",
    "        clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "        scores = cross_val_score(clf, X, y, cv=5)\n",
    "        score_list.append(scores.mean())\n",
    "    return score_list\n",
    "\n",
    "def select_name(names):\n",
    "    A = {}\n",
    "    scores = opti_band(names)\n",
    "    ind = scores.index(max(scores))\n",
    "    name = names[ind]\n",
    "    A[\"name\"] = name\n",
    "    A[\"scores\"] = scores\n",
    "    return A\n",
    "# input_name = 'poi_all_' # \n",
    "# names = get_name(input_name)[0:2]\n",
    "# name = select_name(names)\n",
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'poi_sel_500', 'scores': [0.8580399328355434, 0.8602147154954132]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name = 'poi_sel_'\n",
    "names = get_name(input_name)[0:2]\n",
    "name = select_name(names)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428963100511668\n"
     ]
    }
   ],
   "source": [
    "X_Train =  np.array(Train[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Train = Train['Target'].copy()\n",
    "X_Test = np.array(Test[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Test = Test['Target'].copy() # get sub dataframe\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "\n",
    "clf.fit(X_Train, Y_Train)\n",
    "predictions = clf.predict(X_Test)\n",
    "acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917987554644888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_Train =  np.array(Train[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Train = Train['Target'].copy()\n",
    "X_Test = np.array(Test[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Test = Test['Target'].copy() # get sub dataframe\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_Train, Y_Train)\n",
    "predictions = clf.predict(X_Test)\n",
    "acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8896042283938042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_Train =  np.array(Train[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Train = Train['Target'].copy()\n",
    "X_Test = np.array(Test[['poi_sel_250', 'RN_500','NTL']])\n",
    "Y_Test = Test['Target'].copy() # get sub dataframe\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0)\n",
    "\n",
    "clf.fit(X_Train, Y_Train)\n",
    "predictions = clf.predict(X_Test)\n",
    "acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation3\n",
    "\n",
    "input_file_loc = 'ASCII'\n",
    "ly_name = 'Train_Test'\n",
    "read_ASC_Data(file_name, ly_names):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Urban Built-up Areas\n",
    "\n",
    "### import sklearn libraries and read data\n",
    "\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sl_1 import * # my custom module \n",
    "from sklearn.metrics import accuracy_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ly_df = pd.read_csv(r'data\\ly_df.csv')\n",
    "ly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "def get_comb(input_list,n):\n",
    "    '''input: input_list, a list.\n",
    "       input: n，an integer; the number of combinations.\n",
    "       output: a list, the combinations of input_list.\n",
    "    '''\n",
    "    comb_all = []\n",
    "    m = 1\n",
    "    while m<n+1:\n",
    "        a = [list(i) for i in list(combinations(input_list, m))]\n",
    "        comb_all += a\n",
    "        m += 1\n",
    "    return comb_all\n",
    "comb_all = get_comb([1,2,3], 3)\n",
    "comb_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time : Query time, but the variable is not saved\n",
    "\n",
    "def get_acc(ind,node_num):\n",
    "#     x_features = clean_data.iloc[:,1:ind].columns\n",
    "    x_features = clean_data.iloc[:,ind].columns\n",
    "    y_col = 'Target'\n",
    "    X_Train = Train[x_features].copy()\n",
    "    X_Test = Test[x_features].copy()\n",
    "    Y_Train = Train[[y_col]].copy()\n",
    "    Y_Test = Test[[y_col]].copy() # get sub dataframe\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=node_num, random_state=1)\n",
    "    clf.fit(X_Train, Y_Train)\n",
    "    predictions = clf.predict(X_Test)\n",
    "#     acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true = Y_Test, y_pred = predictions).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return f1_score\n",
    "# Permutations\n",
    "# l = [[get_acc(i,j) for i in get_comb([1,2,3], 3)] for j in range(2,51)]\n",
    "def acc_node(a,b): # a=2,b=51\n",
    "    dict1 = {}\n",
    "    for col_inx in get_comb([1,2,3], 3):\n",
    "        list1 = []\n",
    "        for j in range(a,b):\n",
    "            list1.append(get_acc(col_inx,j))\n",
    "            if len(col_inx) <= 1:\n",
    "                ASC_Name = clean_data.columns[col_inx][0]\n",
    "            else:\n",
    "                ASC_Name = '_'.join(clean_data.columns[col_inx])\n",
    "        dict1[ASC_Name] = list1\n",
    "    return dict1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "DT_node = acc_node(2,31)\n",
    "t2 = time.time()\n",
    "print('finised..%s min' % (round((t2-t1)/60,2)))\n",
    "# time: I use 9.8 min \n",
    "\n",
    "DT_nodes = pd.DataFrame(DT_node,index =list(range(2,31)))\n",
    "DT_nodes.to_csv(r'data\\DT_node.csv',index_label = 'nodes')\n",
    "# DT_nodes = pd.read_csv(r'data\\DT_node.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(a, b): # a: true target; b: predict\n",
    "    arr1 = np.zeros(shape=(len(a),1))\n",
    "    arr1[(b == 0) & (a == 0)] = 0 # TN\n",
    "    arr1[(b == 1) & (a == 1)] = 1 # TP\n",
    "    arr1[(b == 1) & (a == 0)] = 2 # FP\n",
    "    arr1[(b == 0) & (a == 1)] = 3 # FN\n",
    "    return arr1\n",
    "# a = np.array([0,0,1,1])\n",
    "# b = np.array([0,1,1,0])\n",
    "# function(a, b)\n",
    "def para_dict(clean_data, col_inx, node_num):\n",
    "    '''input: clean_data, a cleaned dataset (no NA) used to predict the built area.\n",
    "       input: col_inx, a columns index list of cleaned data used to predict urban built.\n",
    "       input: node_num, a number of nodes of decison tree.\n",
    "       output: the predicted result and parameter of decision tree.\n",
    "    '''\n",
    "    dict0 = {}\n",
    "    x_features = clean_data.iloc[:,col_inx].columns\n",
    "    y_col = 'Target'\n",
    "    X_Train = Train[x_features].copy()\n",
    "    X_Test = Test[x_features].copy()\n",
    "    Y_Train = Train[[y_col]].copy()\n",
    "    Y_Test = Test[[y_col]].copy() # get sub dataframe\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=node_num, random_state=1)\n",
    "    clf.fit(X_Train, Y_Train)\n",
    "    predictions = clf.predict(X_Test)\n",
    "# test in test data\n",
    "#     acc1 = accuracy_score(y_true = Y_Test, y_pred = predictions) # accuracy of test data\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true = Y_Test, y_pred = predictions).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    proba = clf.predict_proba(X_Test)\n",
    "    log_proba = clf.predict_log_proba(X_Test)\n",
    "\n",
    "# prediction\n",
    "    x_data = clean_data[x_features].copy()\n",
    "    y_data = clean_data[[y_col]].copy()\n",
    "    pred_y = clf.predict(x_data)\n",
    "# over_all accuracy\n",
    "#     acc2 = accuracy_score(y_true = y_data, y_pred = pred_y)\n",
    "#     overall_kappa = cohen_kappa_score(y_data, pred_y)\n",
    "#     proba = clf.predict_proba(x_data)\n",
    "#     log_proba = clf.predict_log_proba(x_data)\n",
    "\n",
    "    data1 = clean_data.copy()\n",
    "    ins_loc = len(clean_data.columns) # insert it to end\n",
    "    data1.insert(ins_loc,'pred_y',pred_y)\n",
    "    a = np.array(data1.Target)\n",
    "    b = pred_y\n",
    "    data1['pred_y1'] = function(a, b)\n",
    "    \n",
    "    if len(col_inx) <= 1:\n",
    "        ASC_Name = clean_data.columns[col_inx][0]\n",
    "    else:\n",
    "        ASC_Name = '_'.join(clean_data.columns[col_inx])\n",
    "### add to dictionary\n",
    "    dict0['Name'] =  ASC_Name\n",
    "    dict0['Precision'] = round(precision,6) # pre\n",
    "    dict0['F1_score'] = round(f1_score,6) # pre\n",
    "    dict0['Proba'] = proba\n",
    "    dict0['Log_Proba'] = log_proba\n",
    "    dict0['Recall'] = round(recall,6)\n",
    "    dict0['Pred_y'] = pred_y\n",
    "    dict0['Pred_y1'] = np.array(data1['pred_y1'])\n",
    "#     dict0['Acc_Test'] = round(acc1,6)\n",
    "#     dict0['Acc_All'] = round(acc2,6)\n",
    "#     dict0['Kappa'] = round(overall_kappa,6)\n",
    "#     dict0['Target'] = np.array(y_data).ravel() # overall\n",
    "    dict0['Target'] = np.array(Y_Test).ravel()\n",
    "    dict0['TN'] = tn\n",
    "    dict0['FP'] = fp\n",
    "    dict0['FN'] = fn\n",
    "    dict0['TP'] = tp\n",
    "    return dict0\n",
    "# %%time\n",
    "# 5.93 s\n",
    "# para_0 = para_dict(clean_data, [1,2], 11)\n",
    "# para_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 27.1 s\n",
    "def para_dict_all(data, col_inx_list, node_num):\n",
    "    '''input: col_inx, a list, a columns index list of cleaned data used to predict urban built.\n",
    "       input: node_num, a number of nodes of decison tree.\n",
    "       output: the predicted result and parameter of decision tree.\n",
    "    '''\n",
    "    dict1 = {}\n",
    "    i = 0\n",
    "    for col_inx in col_inx_list:\n",
    "        para_0 = para_dict(data, col_inx, node_num)\n",
    "        dict1[str(i)] = para_0\n",
    "        i += 1\n",
    "    return dict1\n",
    "# para_DT = para_dict_all(clean_data, [[1],[2],[3],[1,2,3]], 11)\n",
    "# para_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 17.9 s\n",
    "def pred_ASC(raw_data, clean_data, para_all, w_name, Bool=1):\n",
    "    '''input:raw_data, a raw dataset (have NA) used to predict the built area.\n",
    "       input:clean_data, a cleaned dataset (no NA) used to predict the built area.\n",
    "       input:para_all, a dict, the predicted result and parameter of decision tree.\n",
    "       input:w_name, the output file folder name.\n",
    "       input:Bool, a bool value of '0' or '1', defult is 1; '0' means pred y have \n",
    "             value(0 and 1) while '1' means pred y have  value(0, 1, 2 and 3).\n",
    "       output: print the pred_y to a ASCII file.\n",
    "    '''\n",
    "    print('The program starts running')\n",
    "    for key,value in para_all.items():\n",
    "        if key != 'Target':\n",
    "            name = value['Name']\n",
    "            if Bool == 1:\n",
    "                pred_y = value['Pred_y1']\n",
    "            elif Bool == 0:\n",
    "                pred_y = value['Pred_y']\n",
    "            else:\n",
    "                print('unexpected input variable of Bool. Bool are suposed to be a bool value of \"0\" or \"1\" ')\n",
    "            data1 = clean_data.copy()\n",
    "            data1['pred_y'] = pred_y\n",
    "            data2 = pd.merge(raw_data, data1.iloc[:,[-2,-1]], how='left', left_on='ID', right_on='ID', sort=True)\n",
    "            data3 = data2.copy().fillna(-9999)\n",
    "            shape_ras = (1786, 1932) # Number of rows and columns\n",
    "            built_pre = np.array(data3['pred_y']).reshape(shape_ras)\n",
    "            np.savetxt('data/%s/%s.txt' % (w_name,name), built_pre, fmt='%0.0f') # Skip the first 6 lines\n",
    "            add_begin('%s/%s' % (w_name,name))\n",
    "        else:\n",
    "            pass\n",
    "    print('The program has finished running')\n",
    "# para_DT = para_dict_all(clean_data, [[1],[2],[3],[1,2,3]], 11)\n",
    "# pred_ASC(raw_data, clean_data, para_DT, pred_DT,Bool = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>RN</th>\n",
       "      <th>NTL</th>\n",
       "      <th>POI</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target   RN  NTL  POI  XM_Boundary    ID\n",
       "0     0.0  0.0  0.0  0.0          1.0   430\n",
       "1     0.0  0.0  0.0  0.0          1.0   431\n",
       "2     0.0  0.0  0.0  0.0          1.0  2361\n",
       "3     0.0  0.0  0.0  0.0          1.0  2362\n",
       "4     0.0  0.0  0.0  0.0          1.0  2363"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(r'data\\ly_df.csv')\n",
    "# raw_data.head()\n",
    "clean_data = pd.read_csv(r'data\\ly_df_clean.csv')\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train, Test = train_test_split(clean_data, test_size=0.33, random_state=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predicted data to ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to ASCII\n",
    "- NTL.txt\n",
    "- POI.txt\n",
    "- RN.txt\n",
    "- RN_NTL_POI.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program starts running\n",
      "The program has finished running\n"
     ]
    }
   ],
   "source": [
    "para_DT = para_dict_all(clean_data, [[1],[2],[3],[1,2,3]], 11)\n",
    "pred_ASC(raw_data, clean_data, para_DT, 'pred_DT',Bool = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save parameter to csv\n",
    "- precision\n",
    "- recall\n",
    "- f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_DT['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_para = pd.DataFrame(para_DT).T\n",
    "# df_para.head()\n",
    "# order = ['Name','Acc_Test','Acc_All','Kappa','TN','TP','FN','FP','Precision','Recall']\n",
    "order = ['Name','TN','TP','FN','FP','Precision','Recall','F1_score']\n",
    "df_para = df_para[order]\n",
    "df_para.to_csv('data/pred_DT/para_DT.csv', index = False)\n",
    "# new_para = pd.read_csv('data/pred_DT/para_DT.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.042px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
