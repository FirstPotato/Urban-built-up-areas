{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data introduction and import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sl_1 import * # my custom module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resolution:** 30m * 30m<br>\n",
    "**Target:** Target data set<br>\n",
    "**Point of Interest (POI):** BandWidth of POI KDE, POI_All: 1000m; POI_Sel: 500m<br>\n",
    "**Road Network (RN):** BandWidth1250m<br>\n",
    "**NTL:** Time,2019/03; NPP-VIIRS, DNB<br>\n",
    "**XM_Boundary:** mask of all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate header named begin.txt from layer Target\n"
     ]
    }
   ],
   "source": [
    "# Extract and generate headers\n",
    "extrat_begin('Target', 'begin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ASCII to array and generate header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def read_ASC_Data(file_name, ly_names):\n",
    "    '''input: file_name, a file name, parent file name of ly_names\n",
    "       input: ly_names, a list, list of layer names in an ASCII data format\n",
    "       output: ly_dict: a dictionary whose key is the layer name and the value is stored in an array format\n",
    "    '''     \n",
    "    ly_dict = {}\n",
    "    for name in ly_names:\n",
    "        ly_dict[name] = np.loadtxt('%s/\\%s.txt' % (file_name, name), skiprows = 6)\n",
    "    print('All ASCII data has been read')\n",
    "    return ly_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data consistency test\n",
    "Check whether there are missing values in different positions of the layer,\n",
    "and auto fill it if missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Na_Test(layer_names,stan_layer,ly_dict):\n",
    "    '''input: layer_names, a list of layer names to be checked for missing values\n",
    "       input: stan_layer, the normalized layer name used to verify that other layers have missing values\n",
    "       ly_dict: the dict data read by read_ASC_Data function.\n",
    "       output: For each layer in the name list, the following judgment is made. \n",
    "               If all the row raster cells of a layer are missing from the standard layer, \n",
    "               the layer is added to the result return list, \n",
    "               otherwise, the next layer is performed. Judgment.\n",
    "    '''\n",
    "    list1 = []\n",
    "    for name in layer_names:\n",
    "        # Fill in missing values start\n",
    "        ly_dict[name][np.where(ly_dict[name] == -9999)] = 0 # Fill all missing values (- 9999) with 0\n",
    "        ly_dict[name][np.where(ly_dict[stan_layer] == -9999)] = -9999 # Make - 9999 consistent with the standardized layer\n",
    "        # Fill in missing values end\n",
    "        a = max(ly_dict[name][np.where(ly_dict[stan_layer] == -9999)])\n",
    "        b = min(ly_dict[name][np.where(ly_dict[stan_layer] != -9999)])\n",
    "        if a == -9999 and b != -9999:\n",
    "            continue\n",
    "        else:\n",
    "            list1.append(name)\n",
    "    if len(list1) == 0:\n",
    "        print('All layers pass the inspection, no missing values exist, and are consistent with the standardized layer')\n",
    "    else:\n",
    "        print('The following layers have missing values')\n",
    "        print(list1)\n",
    "    return\n",
    "### test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revel_array(dict_one):\n",
    "    '''input: dict_one, a dictionary whose key is the layer name and the value is stored in an array format\n",
    "       output: A dict whose value is all converted to one-dimensional array\n",
    "    '''\n",
    "    new_dict = copy.deepcopy(dict_one)\n",
    "    for key,value in new_dict.items():\n",
    "        new_dict[key] = np.ravel(value, order='C') # Expand to one dimension by row\n",
    "    num_rows = len(list(new_dict.values())[0])\n",
    "    new_dict['ID'] = np.array([i for i in range(num_rows)]) # Build index with name ID\n",
    "    return new_dict\n",
    "\n",
    "# ly_dict_reval = revel_array(ly_dict)\n",
    "# ly_dict_reval\n",
    "# len(ly_dict_reval['Target']) # show total number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conversion to dataframe and to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ly_df_csv(input_file_loc,ly_names,write_name):\n",
    "    '''\n",
    "    input: input_file_loc, file loction name of input data\n",
    "    input: ly_names, , a list of layer names to be combined\n",
    "    output: a csv file named write_name\n",
    "    '''\n",
    "    ly_dict0 = read_ASC_Data(input_file_loc, ly_names) # read ASCII data\n",
    "    ly_dict = copy.deepcopy(ly_dict0)\n",
    "#     print(ly_dict['Target'])\n",
    "#     print(np.shape(ly_dict['Target']))\n",
    "    test_name = ly_names\n",
    "    Na_Test(test_name[:-1],test_name[-1], ly_dict) # test_name[-1], stan_layer, e.g.'XM_Boundary'\n",
    "    ly_dict_reval = revel_array(ly_dict) # Dimension reduction\n",
    "#     print(len(ly_dict_reval['Target'])) # show total number of records\n",
    "    ly_df = pd.DataFrame(ly_dict_reval,columns=ly_dict_reval.keys())\n",
    "    ly_df.to_csv(r'data\\%s.csv' % (write_name),index = False)\n",
    "    print('Data writing is over')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write data to ly_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ASCII data has been read\n",
      "All layers pass the inspection, no missing values exist, and are consistent with the standardized layer\n"
     ]
    }
   ],
   "source": [
    "input_file_loc = 'ASCII'\n",
    "ly_names = ['Target','POI','RN','NTL','XM_Boundary']\n",
    "write_name = 'ly_df'\n",
    "ly_df_csv(input_file_loc,ly_names,write_name) # write the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Urban Built-up Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import sklearn libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sl_1 import * # my custom module \n",
    "from sklearn.metrics import accuracy_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>POI</th>\n",
       "      <th>RN</th>\n",
       "      <th>NTL</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target     POI      RN     NTL  XM_Boundary  ID\n",
       "0 -9999.0 -9999.0 -9999.0 -9999.0      -9999.0   0\n",
       "1 -9999.0 -9999.0 -9999.0 -9999.0      -9999.0   1\n",
       "2 -9999.0 -9999.0 -9999.0 -9999.0      -9999.0   2\n",
       "3 -9999.0 -9999.0 -9999.0 -9999.0      -9999.0   3\n",
       "4 -9999.0 -9999.0 -9999.0 -9999.0      -9999.0   4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ly_df = pd.read_csv(r'data\\ly_df.csv')\n",
    "ly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Target', 'POI', 'RN', 'NTL', 'XM_Boundary', 'ID'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ly_df.copy()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>POI</th>\n",
       "      <th>RN</th>\n",
       "      <th>NTL</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target  POI   RN  NTL  XM_Boundary    ID\n",
       "430      0.0  0.0  0.0  0.0          1.0   430\n",
       "431      0.0  0.0  0.0  0.0          1.0   431\n",
       "2361     0.0  0.0  0.0  0.0          1.0  2361\n",
       "2362     0.0  0.0  0.0  0.0          1.0  2362\n",
       "2363     0.0  0.0  0.0  0.0          1.0  2363"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = data.copy()\n",
    "clean_data = clean_data[clean_data.loc[:,'Target'] != -9999]\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder and save clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "order = ['Target', 'RN', 'NTL', 'POI', 'XM_Boundary', 'ID']\n",
    "clean_data = clean_data[order]\n",
    "# clean_data.rename(columns={'POI_Sel':'POI'}, inplace = True)\n",
    "clean_data.head()\n",
    "clean_data.to_csv(r'data\\ly_df_clean.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read clean data and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>RN</th>\n",
       "      <th>NTL</th>\n",
       "      <th>POI</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target   RN  NTL  POI  XM_Boundary    ID\n",
       "0     0.0  0.0  0.0  0.0          1.0   430\n",
       "1     0.0  0.0  0.0  0.0          1.0   431\n",
       "2     0.0  0.0  0.0  0.0          1.0  2361\n",
       "3     0.0  0.0  0.0  0.0          1.0  2362\n",
       "4     0.0  0.0  0.0  0.0          1.0  2363"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = pd.read_csv(r'data\\ly_df_clean.csv')\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1890571"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data) # Sample size: 1890571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train, Test = train_test_split(clean_data, test_size=0.33, random_state=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "def get_comb(input_list,n):\n",
    "    '''input: input_list, a list.\n",
    "       input: n，an integer; the number of combinations.\n",
    "       output: a list, the combinations of input_list.\n",
    "    '''\n",
    "    comb_all = []\n",
    "    m = 1\n",
    "    while m<n+1:\n",
    "        a = [list(i) for i in list(combinations(input_list, m))]\n",
    "        comb_all += a\n",
    "        m += 1\n",
    "    return comb_all\n",
    "comb_all = get_comb([1,2,3], 3)\n",
    "comb_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time : Query time, but the variable is not saved\n",
    "\n",
    "def get_acc(ind,node_num):\n",
    "#     x_features = clean_data.iloc[:,1:ind].columns\n",
    "    x_features = clean_data.iloc[:,ind].columns\n",
    "    y_col = 'Target'\n",
    "    X_Train = Train[x_features].copy()\n",
    "    X_Test = Test[x_features].copy()\n",
    "    Y_Train = Train[[y_col]].copy()\n",
    "    Y_Test = Test[[y_col]].copy() # get sub dataframe\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=node_num, random_state=1)\n",
    "    clf.fit(X_Train, Y_Train)\n",
    "    predictions = clf.predict(X_Test)\n",
    "#     acc = accuracy_score(y_true = Y_Test, y_pred = predictions) # use f1-score instead\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true = Y_Test, y_pred = predictions).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return f1_score\n",
    "# Permutations\n",
    "# l = [[get_acc(i,j) for i in get_comb([1,2,3], 3)] for j in range(2,51)]\n",
    "def acc_node(a,b): # a=2,b=51\n",
    "    dict1 = {}\n",
    "    for col_inx in get_comb([1,2,3], 3):\n",
    "        list1 = []\n",
    "        for j in range(a,b):\n",
    "            list1.append(get_acc(col_inx,j))\n",
    "            if len(col_inx) <= 1:\n",
    "                ASC_Name = clean_data.columns[col_inx][0]\n",
    "            else:\n",
    "                ASC_Name = '_'.join(clean_data.columns[col_inx])\n",
    "        dict1[ASC_Name] = list1\n",
    "    return dict1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "DT_node = acc_node(2,31)\n",
    "t2 = time.time()\n",
    "print('finised..%s min' % (round((t2-t1)/60,2)))\n",
    "# time: I use 9.8 min \n",
    "\n",
    "DT_nodes = pd.DataFrame(DT_node,index =list(range(2,31)))\n",
    "DT_nodes.to_csv(r'data\\DT_node.csv',index_label = 'nodes')\n",
    "# DT_nodes = pd.read_csv(r'data\\DT_node.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(a, b): # a: true target; b: predict\n",
    "    arr1 = np.zeros(shape=(len(a),1))\n",
    "    arr1[(b == 0) & (a == 0)] = 0 # TN\n",
    "    arr1[(b == 1) & (a == 1)] = 1 # TP\n",
    "    arr1[(b == 1) & (a == 0)] = 2 # FP\n",
    "    arr1[(b == 0) & (a == 1)] = 3 # FN\n",
    "    return arr1\n",
    "# a = np.array([0,0,1,1])\n",
    "# b = np.array([0,1,1,0])\n",
    "# function(a, b)\n",
    "def para_dict(clean_data, col_inx, node_num):\n",
    "    '''input: clean_data, a cleaned dataset (no NA) used to predict the built area.\n",
    "       input: col_inx, a columns index list of cleaned data used to predict urban built.\n",
    "       input: node_num, a number of nodes of decison tree.\n",
    "       output: the predicted result and parameter of decision tree.\n",
    "    '''\n",
    "    dict0 = {}\n",
    "    x_features = clean_data.iloc[:,col_inx].columns\n",
    "    y_col = 'Target'\n",
    "    X_Train = Train[x_features].copy()\n",
    "    X_Test = Test[x_features].copy()\n",
    "    Y_Train = Train[[y_col]].copy()\n",
    "    Y_Test = Test[[y_col]].copy() # get sub dataframe\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=node_num, random_state=1)\n",
    "    clf.fit(X_Train, Y_Train)\n",
    "    predictions = clf.predict(X_Test)\n",
    "# test in test data\n",
    "#     acc1 = accuracy_score(y_true = Y_Test, y_pred = predictions) # accuracy of test data\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true = Y_Test, y_pred = predictions).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    proba = clf.predict_proba(X_Test)\n",
    "    log_proba = clf.predict_log_proba(X_Test)\n",
    "\n",
    "# prediction\n",
    "    x_data = clean_data[x_features].copy()\n",
    "    y_data = clean_data[[y_col]].copy()\n",
    "    pred_y = clf.predict(x_data)\n",
    "# over_all accuracy\n",
    "#     acc2 = accuracy_score(y_true = y_data, y_pred = pred_y)\n",
    "#     overall_kappa = cohen_kappa_score(y_data, pred_y)\n",
    "#     proba = clf.predict_proba(x_data)\n",
    "#     log_proba = clf.predict_log_proba(x_data)\n",
    "\n",
    "    data1 = clean_data.copy()\n",
    "    ins_loc = len(clean_data.columns) # insert it to end\n",
    "    data1.insert(ins_loc,'pred_y',pred_y)\n",
    "    a = np.array(data1.Target)\n",
    "    b = pred_y\n",
    "    data1['pred_y1'] = function(a, b)\n",
    "    \n",
    "    if len(col_inx) <= 1:\n",
    "        ASC_Name = clean_data.columns[col_inx][0]\n",
    "    else:\n",
    "        ASC_Name = '_'.join(clean_data.columns[col_inx])\n",
    "### add to dictionary\n",
    "    dict0['Name'] =  ASC_Name\n",
    "    dict0['Precision'] = round(precision,6) # pre\n",
    "    dict0['F1_score'] = round(f1_score,6) # pre\n",
    "    dict0['Proba'] = proba\n",
    "    dict0['Log_Proba'] = log_proba\n",
    "    dict0['Recall'] = round(recall,6)\n",
    "    dict0['Pred_y'] = pred_y\n",
    "    dict0['Pred_y1'] = np.array(data1['pred_y1'])\n",
    "#     dict0['Acc_Test'] = round(acc1,6)\n",
    "#     dict0['Acc_All'] = round(acc2,6)\n",
    "#     dict0['Kappa'] = round(overall_kappa,6)\n",
    "#     dict0['Target'] = np.array(y_data).ravel() # overall\n",
    "    dict0['Target'] = np.array(Y_Test).ravel()\n",
    "    dict0['TN'] = tn\n",
    "    dict0['FP'] = fp\n",
    "    dict0['FN'] = fn\n",
    "    dict0['TP'] = tp\n",
    "    return dict0\n",
    "# %%time\n",
    "# 5.93 s\n",
    "# para_0 = para_dict(clean_data, [1,2], 11)\n",
    "# para_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 27.1 s\n",
    "def para_dict_all(data, col_inx_list, node_num):\n",
    "    '''input: col_inx, a list, a columns index list of cleaned data used to predict urban built.\n",
    "       input: node_num, a number of nodes of decison tree.\n",
    "       output: the predicted result and parameter of decision tree.\n",
    "    '''\n",
    "    dict1 = {}\n",
    "    i = 0\n",
    "    for col_inx in col_inx_list:\n",
    "        para_0 = para_dict(data, col_inx, node_num)\n",
    "        dict1[str(i)] = para_0\n",
    "        i += 1\n",
    "    return dict1\n",
    "# para_DT = para_dict_all(clean_data, [[1],[2],[3],[1,2,3]], 11)\n",
    "# para_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Wall time: 17.9 s\n",
    "def pred_ASC(raw_data, clean_data, para_all, w_name, Bool=1):\n",
    "    '''input:raw_data, a raw dataset (have NA) used to predict the built area.\n",
    "       input:clean_data, a cleaned dataset (no NA) used to predict the built area.\n",
    "       input:para_all, a dict, the predicted result and parameter of decision tree.\n",
    "       input:w_name, the output file folder name.\n",
    "       input:Bool, a bool value of '0' or '1', defult is 1; '0' means pred y have \n",
    "             value(0 and 1) while '1' means pred y have  value(0, 1, 2 and 3).\n",
    "       output: print the pred_y to a ASCII file.\n",
    "    '''\n",
    "    print('The program starts running')\n",
    "    for key,value in para_all.items():\n",
    "        if key != 'Target':\n",
    "            name = value['Name']\n",
    "            if Bool == 1:\n",
    "                pred_y = value['Pred_y1']\n",
    "            elif Bool == 0:\n",
    "                pred_y = value['Pred_y']\n",
    "            else:\n",
    "                print('unexpected input variable of Bool. Bool are suposed to be a bool value of \"0\" or \"1\" ')\n",
    "            data1 = clean_data.copy()\n",
    "            data1['pred_y'] = pred_y\n",
    "            data2 = pd.merge(raw_data, data1.iloc[:,[-2,-1]], how='left', left_on='ID', right_on='ID', sort=True)\n",
    "            data3 = data2.copy().fillna(-9999)\n",
    "            shape_ras = (1786, 1932) # Number of rows and columns\n",
    "            built_pre = np.array(data3['pred_y']).reshape(shape_ras)\n",
    "            np.savetxt('data/%s/%s.txt' % (w_name,name), built_pre, fmt='%0.0f') # Skip the first 6 lines\n",
    "            add_begin('%s/%s' % (w_name,name))\n",
    "        else:\n",
    "            pass\n",
    "    print('The program has finished running')\n",
    "# para_DT = para_dict_all(clean_data, [[1],[2],[3],[1,2,3]], 11)\n",
    "# pred_ASC(raw_data, clean_data, para_DT, pred_DT,Bool = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>RN</th>\n",
       "      <th>NTL</th>\n",
       "      <th>POI</th>\n",
       "      <th>XM_Boundary</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target   RN  NTL  POI  XM_Boundary    ID\n",
       "0     0.0  0.0  0.0  0.0          1.0   430\n",
       "1     0.0  0.0  0.0  0.0          1.0   431\n",
       "2     0.0  0.0  0.0  0.0          1.0  2361\n",
       "3     0.0  0.0  0.0  0.0          1.0  2362\n",
       "4     0.0  0.0  0.0  0.0          1.0  2363"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(r'data\\ly_df.csv')\n",
    "# raw_data.head()\n",
    "clean_data = pd.read_csv(r'data\\ly_df_clean.csv')\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train, Test = train_test_split(clean_data, test_size=0.33, random_state=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predicted data to ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to ASCII\n",
    "- NTL.txt\n",
    "- POI.txt\n",
    "- RN.txt\n",
    "- RN_NTL_POI.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program starts running\n",
      "The program has finished running\n"
     ]
    }
   ],
   "source": [
    "para_DT = para_dict_all(clean_data, [[1],[2],[3],[1,2,3]], 11)\n",
    "pred_ASC(raw_data, clean_data, para_DT, 'pred_DT',Bool = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save parameter to csv\n",
    "- precision\n",
    "- recall\n",
    "- f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_DT['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_para = pd.DataFrame(para_DT).T\n",
    "# df_para.head()\n",
    "# order = ['Name','Acc_Test','Acc_All','Kappa','TN','TP','FN','FP','Precision','Recall']\n",
    "order = ['Name','TN','TP','FN','FP','Precision','Recall','F1_score']\n",
    "df_para = df_para[order]\n",
    "df_para.to_csv('data/pred_DT/para_DT.csv', index = False)\n",
    "# new_para = pd.read_csv('data/pred_DT/para_DT.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.058px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
